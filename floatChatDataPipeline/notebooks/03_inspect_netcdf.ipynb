{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eccbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbec091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 NetCDF files:\n",
      "  0: ISAS15_SSS_20020115_fld_PSAL.nc\n",
      "  1: ISAS15_SSS_20031215_fld_PSAL.nc\n",
      "  2: ISAS20_ARGO_20110615_dat_DOXY.nc\n",
      "\n",
      "Loading: ISAS20_ARGO_20110615_dat_DOXY.nc\n",
      "Attempting to load with time decoding...\n",
      "⚠ Time decoding failed, loading without time decoding...\n",
      "✓ Successfully loaded without time decoding\n"
     ]
    }
   ],
   "source": [
    "def load_netcdf_safe(file_path, decode_times_first=True):\n",
    "    \"\"\"\n",
    "    Safely load NetCDF file with automatic fallback for time decoding issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if decode_times_first:\n",
    "            print(\"Attempting to load with time decoding...\")\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            print(\"✓ Successfully loaded with time decoding\")\n",
    "        else:\n",
    "            raise ValueError(\"Skip first attempt\")\n",
    "            \n",
    "    except (ValueError, OSError) as e:\n",
    "        if \"time units\" in str(e) or \"REFERENCE_DATE_TIME\" in str(e):\n",
    "            print(\"⚠ Time decoding failed, loading without time decoding...\")\n",
    "            ds = xr.open_dataset(file_path, decode_times=False)\n",
    "            print(\"✓ Successfully loaded without time decoding\")\n",
    "        else:\n",
    "            print(f\"Error loading file: {e}\")\n",
    "            return None\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Example usage\n",
    "DATA_DIR = '../NetCDFDatafile/'\n",
    "\n",
    "# List available files\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.endswith('.nc')]\n",
    "print(f\"Found {len(files)} NetCDF files:\")\n",
    "for i, file in enumerate(files[:5]):  # Show first 5 files\n",
    "    print(f\"  {i}: {file}\")\n",
    "\n",
    "# Select and load a file\n",
    "if files:\n",
    "    file_index = min(2, len(files)-1)  # Use index 2 or last available\n",
    "    sample_file = files[file_index]\n",
    "    file_path = os.path.join(DATA_DIR, sample_file)\n",
    "    \n",
    "    print(f\"\\nLoading: {sample_file}\")\n",
    "    ds = load_netcdf_safe(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f47a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "<xarray.Dataset> Size: 181MB\n",
      "Dimensions:              (N_PROF: 33635, N_LEVELS: 187)\n",
      "Coordinates:\n",
      "    JULD_datetime        (N_PROF) datetime64[ns] 269kB 2013-02-06T15:22:30 .....\n",
      "Dimensions without coordinates: N_PROF, N_LEVELS\n",
      "Data variables: (12/21)\n",
      "    REFERENCE_DATE_TIME  |S16 16B ...\n",
      "    DATA_TYPE            |S16 16B ...\n",
      "    PLATFORM_NUMBER      (N_PROF) |S8 269kB ...\n",
      "    WMO_INST_TYPE        (N_PROF) |S4 135kB ...\n",
      "    PI_NAME              (N_PROF) |S64 2MB ...\n",
      "    DATA_CENTRE          (N_PROF) |S2 67kB ...\n",
      "    ...                   ...\n",
      "    DOXY                 (N_PROF, N_LEVELS) float32 25MB ...\n",
      "    DOXY_CLMN            (N_PROF, N_LEVELS) float32 25MB ...\n",
      "    DOXY_CLSD            (N_PROF, N_LEVELS) float32 25MB ...\n",
      "    DOXY_ERME            (N_PROF, N_LEVELS) float32 25MB ...\n",
      "    DOXY_ERUR            (N_PROF, N_LEVELS) float32 25MB ...\n",
      "    DOXY_RESI            (N_PROF, N_LEVELS) float32 25MB ...\n",
      "Attributes:\n",
      "    Conventions:       CF-1.4\n",
      "    title:             Yearly mean 2009-2013\n",
      "    institution:       LOPS/OSU-IUEM/Ifremer\n",
      "    project_name:      SNO Argo-France\n",
      "    data_manager:      Nicolas Kolodziejczyk(nicolas.kolodziejczyk@univ-brest...\n",
      "    software_version:  POSTOA_main - 7.0\n",
      "    references:        Gaillard et al., JCLIM 2016, Doi:10.1175/JCLI-D-15-002...\n",
      "    history:           20221219T162912L : Creation\n",
      "    data_doi:          https://doi.org/10.17882/52367\n",
      "\n",
      "============================================================\n",
      "BASIC INFORMATION\n",
      "============================================================\n",
      "File size: 172.64 MB\n",
      "Dimensions: {'N_PROF': 33635, 'N_LEVELS': 187}\n",
      "Variables: 21\n",
      "Coordinates: 1\n",
      "\n",
      "============================================================\n",
      "VARIABLES SUMMARY\n",
      "============================================================\n",
      "REFERENCE_DATE_TIME | Shape: ()                   | Type: |S16\n",
      "                | Description: UTC-Date-time of reference for Julian days\n",
      "\n",
      "DATA_TYPE       | Shape: ()                   | Type: |S16\n",
      "                | Description: Data type\n",
      "\n",
      "PLATFORM_NUMBER | Shape: (33635,)             | Type: |S8\n",
      "                | Description: Float unique identifier\n",
      "\n",
      "WMO_INST_TYPE   | Shape: (33635,)             | Type: |S4\n",
      "                | Description: Coded instrument type\n",
      "\n",
      "PI_NAME         | Shape: (33635,)             | Type: |S64\n",
      "                | Description: Name of the principal investigator\n",
      "\n",
      "DATA_CENTRE     | Shape: (33635,)             | Type: |S2\n",
      "                | Description: Data centre in charge of float data processing\n",
      "\n",
      "DC_REFERENCE    | Shape: (33635,)             | Type: |S32\n",
      "                | Description: Unique identifier for profile/in data centre\n",
      "\n",
      "DATA_MODE       | Shape: (33635,)             | Type: |S1\n",
      "                | Description: Delayed mode or real time data\n",
      "\n",
      "CYCLE_NUMBER    | Shape: (33635,)             | Type: int16\n",
      "                | Description: Cycle number\n",
      "\n",
      "JULD            | Shape: (33635,)             | Type: float64\n",
      "                | Description: Julian day (UTC) relative to REFERENCE_DATE_TIME\n",
      "\n",
      "LATITUDE        | Shape: (33635,)             | Type: float64\n",
      "                | Description: Latitude of the station, best estimate\n",
      "\n",
      "LONGITUDE       | Shape: (33635,)             | Type: float64\n",
      "                | Description: Longitude of the station, best estimate\n",
      "\n",
      "DEPH            | Shape: (187,)               | Type: float64\n",
      "\n",
      "DOXY_PROC       | Shape: (33635,)             | Type: int8\n",
      "                | Description: profile processing level\n",
      "\n",
      "DOXY_QC         | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Quality flag on interpolated variable\n",
      "\n",
      "DOXY            | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Disolved oxygen (interpolated on Z_levels)\n",
      "\n",
      "DOXY_CLMN       | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Climatology mean for profile\n",
      "\n",
      "DOXY_CLSD       | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Climatology standard deviation for profile\n",
      "\n",
      "DOXY_ERME       | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Measurement error\n",
      "\n",
      "DOXY_ERUR       | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Error from unresolved scales\n",
      "\n",
      "DOXY_RESI       | Shape: (33635, 187)         | Type: float32\n",
      "                | Description: Residual\n",
      "\n",
      "============================================================\n",
      "COORDINATES\n",
      "============================================================\n",
      "JULD_datetime   | Shape: (33635,)             | Type: datetime64[ns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_dataset(ds):\n",
    "    \"\"\"Comprehensive dataset exploration\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*60)\n",
    "    print(ds)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASIC INFORMATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"File size: {ds.nbytes / 1024**2:.2f} MB\")\n",
    "    print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "    print(f\"Variables: {len(ds.data_vars)}\")\n",
    "    print(f\"Coordinates: {len(ds.coords)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VARIABLES SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for var_name, var in ds.data_vars.items():\n",
    "        print(f\"{var_name:15} | Shape: {str(var.shape):20} | Type: {var.dtype}\")\n",
    "        if hasattr(var, 'long_name'):\n",
    "            print(f\"{'':15} | Description: {var.long_name}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COORDINATES\")\n",
    "    print(\"=\"*60)\n",
    "    for coord_name, coord in ds.coords.items():\n",
    "        print(f\"{coord_name:15} | Shape: {str(coord.shape):20} | Type: {coord.dtype}\")\n",
    "        if coord_name in ['latitude', 'longitude', 'lat', 'lon']:\n",
    "            print(f\"{'':15} | Range: [{coord.min().values:.4f}, {coord.max().values:.4f}]\")\n",
    "        print()\n",
    "\n",
    "# Run exploration\n",
    "if 'ds' in locals() and ds is not None:\n",
    "    explore_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f84286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TIME VARIABLE PROCESSING\n",
      "============================================================\n",
      "Found time variable: JULD\n",
      "  Current units: days since REFERENCE_DATE_TIME\n",
      "  Converting JULD using reference date: 1950-01-01\n",
      "  ✓ Created JULD_datetime\n"
     ]
    }
   ],
   "source": [
    "def handle_juld_time(ds, reference_date='1950-01-01'):\n",
    "    \"\"\"\n",
    "    Convert JULD (Julian Day) variables to proper datetime\n",
    "    \"\"\"\n",
    "    time_vars = ['JULD', 'juld', 'TIME', 'time']\n",
    "    converted_vars = []\n",
    "    \n",
    "    for var_name in time_vars:\n",
    "        if var_name in ds.variables:\n",
    "            print(f\"Found time variable: {var_name}\")\n",
    "            \n",
    "            # Check current units\n",
    "            units = ds[var_name].attrs.get('units', 'No units specified')\n",
    "            print(f\"  Current units: {units}\")\n",
    "            \n",
    "            # If units are problematic, convert manually\n",
    "            if 'REFERENCE_DATE_TIME' in units or 'since' not in units:\n",
    "                print(f\"  Converting {var_name} using reference date: {reference_date}\")\n",
    "                \n",
    "                # Convert to datetime\n",
    "                try:\n",
    "                    # Method 1: Using pandas\n",
    "                    time_converted = pd.to_datetime(ds[var_name].values, \n",
    "                                                  unit='D', \n",
    "                                                  origin=reference_date)\n",
    "                    \n",
    "                    # Create new coordinate\n",
    "                    ds = ds.assign_coords({f'{var_name}_datetime': \n",
    "                                         ([dim for dim in ds[var_name].dims], time_converted)})\n",
    "                    \n",
    "                    # Update attributes\n",
    "                    ds[f'{var_name}_datetime'].attrs['long_name'] = f'{var_name} as datetime'\n",
    "                    ds[f'{var_name}_datetime'].attrs['standard_name'] = 'time'\n",
    "                    ds[f'{var_name}_datetime'].attrs['original_units'] = units\n",
    "                    ds[f'{var_name}_datetime'].attrs['reference_date'] = reference_date\n",
    "                    \n",
    "                    converted_vars.append(f'{var_name}_datetime')\n",
    "                    print(f\"  ✓ Created {var_name}_datetime\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Failed to convert {var_name}: {e}\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"  Units look OK for {var_name}\")\n",
    "    \n",
    "    return ds, converted_vars\n",
    "\n",
    "# Apply JULD conversion\n",
    "if 'ds' in locals() and ds is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TIME VARIABLE PROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    ds, time_vars = handle_juld_time(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a67635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_overview(ds, max_vars=6):\n",
    "    \"\"\"Create overview plots of main variables\"\"\"\n",
    "    \n",
    "    # Get numeric variables only\n",
    "    numeric_vars = []\n",
    "    for var_name, var in ds.data_vars.items():\n",
    "        if np.issubdtype(var.dtype, np.number) and var.size > 1:\n",
    "            numeric_vars.append(var_name)\n",
    "    \n",
    "    # Limit to max_vars for display\n",
    "    vars_to_plot = numeric_vars[:max_vars]\n",
    "    \n",
    "    if not vars_to_plot:\n",
    "        print(\"No suitable variables found for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, var_name in enumerate(vars_to_plot):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        try:\n",
    "            var_data = ds[var_name]\n",
    "            \n",
    "            # Different plot types based on dimensions\n",
    "            if var_data.ndim == 1:\n",
    "                # Line plot for 1D data\n",
    "                var_data.plot(ax=ax)\n",
    "                ax.set_title(f'{var_name} (1D)')\n",
    "                \n",
    "            elif var_data.ndim == 2:\n",
    "                # Heatmap for 2D data\n",
    "                if var_data.size < 10000:  # Only if not too large\n",
    "                    var_data.plot(ax=ax)\n",
    "                    ax.set_title(f'{var_name} (2D)')\n",
    "                else:\n",
    "                    # Sample the data if too large\n",
    "                    sample = var_data.isel({dim: slice(0, min(100, var_data.sizes[dim])) \n",
    "                                          for dim in var_data.dims})\n",
    "                    sample.plot(ax=ax)\n",
    "                    ax.set_title(f'{var_name} (2D - sampled)')\n",
    "                    \n",
    "            else:\n",
    "                # For higher dimensions, plot a slice\n",
    "                slice_dims = {dim: 0 for dim in var_data.dims[:-2]}\n",
    "                var_slice = var_data.isel(slice_dims)\n",
    "                var_slice.plot(ax=ax)\n",
    "                ax.set_title(f'{var_name} (slice)')\n",
    "                \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Error plotting\\n{var_name}:\\n{str(e)[:50]}...', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{var_name} (error)')\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(vars_to_plot), len(axes)):\n",
    "        axes[i].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_geographic_data(ds):\n",
    "    \"\"\"Plot geographic data if lat/lon coordinates exist\"\"\"\n",
    "    \n",
    "    # Look for latitude/longitude coordinates\n",
    "    lat_names = ['latitude', 'lat', 'LATITUDE', 'LAT']\n",
    "    lon_names = ['longitude', 'lon', 'LONGITUDE', 'LON']\n",
    "    \n",
    "    lat_coord = None\n",
    "    lon_coord = None\n",
    "    \n",
    "    for name in lat_names:\n",
    "        if name in ds.coords:\n",
    "            lat_coord = name\n",
    "            break\n",
    "    \n",
    "    for name in lon_names:\n",
    "        if name in ds.coords:\n",
    "            lon_coord = name\n",
    "            break\n",
    "    \n",
    "    if lat_coord and lon_coord:\n",
    "        print(f\"Found geographic coordinates: {lat_coord}, {lon_coord}\")\n",
    "        \n",
    "        # Create geographic plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Scatter plot of positions\n",
    "        ax1.scatter(ds[lon_coord], ds[lat_coord], alpha=0.6, s=20)\n",
    "        ax1.set_xlabel('Longitude')\n",
    "        ax1.set_ylabel('Latitude')\n",
    "        ax1.set_title('Geographic Distribution')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Density plot if enough points\n",
    "        if len(ds[lat_coord]) > 50:\n",
    "            ax2.hist2d(ds[lon_coord], ds[lat_coord], bins=20, alpha=0.8)\n",
    "            ax2.set_xlabel('Longitude')\n",
    "            ax2.set_ylabel('Latitude')\n",
    "            ax2.set_title('Position Density')\n",
    "        else:\n",
    "            ax2.scatter(ds[lon_coord], ds[lat_coord], alpha=0.6, s=50)\n",
    "            ax2.set_xlabel('Longitude')\n",
    "            ax2.set_ylabel('Latitude')\n",
    "            ax2.set_title('All Positions')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print coordinate ranges\n",
    "        print(f\"Latitude range: [{ds[lat_coord].min().values:.4f}, {ds[lat_coord].max().values:.4f}]\")\n",
    "        print(f\"Longitude range: [{ds[lon_coord].min().values:.4f}, {ds[lon_coord].max().values:.4f}]\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No geographic coordinates found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c979ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_variables(ds):\n",
    "    \"\"\"Statistical analysis of dataset variables\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for var_name, var in ds.data_vars.items():\n",
    "        if np.issubdtype(var.dtype, np.number):\n",
    "            print(f\"\\n{var_name}:\")\n",
    "            print(f\"  Shape: {var.shape}\")\n",
    "            print(f\"  Min: {var.min().values}\")\n",
    "            print(f\"  Max: {var.max().values}\")\n",
    "            print(f\"  Mean: {var.mean().values}\")\n",
    "            print(f\"  Std: {var.std().values}\")\n",
    "            \n",
    "            # Count NaN values\n",
    "            if hasattr(var.values, 'isnan'):\n",
    "                nan_count = np.isnan(var.values).sum()\n",
    "                print(f\"  NaN values: {nan_count} ({nan_count/var.size*100:.1f}%)\")\n",
    "\n",
    "def plot_time_series(ds, time_var=None):\n",
    "    \"\"\"Plot time series if time variable exists\"\"\"\n",
    "    \n",
    "    # Find time variable\n",
    "    if time_var is None:\n",
    "        time_candidates = ['JULD_datetime', 'time', 'TIME', 'JULD']\n",
    "        for candidate in time_candidates:\n",
    "            if candidate in ds.coords or candidate in ds.data_vars:\n",
    "                time_var = candidate\n",
    "                break\n",
    "    \n",
    "    if time_var is None:\n",
    "        print(\"No time variable found for time series plotting\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Creating time series plots using: {time_var}\")\n",
    "    \n",
    "    # Get numeric variables for plotting\n",
    "    numeric_vars = [name for name, var in ds.data_vars.items() \n",
    "                   if np.issubdtype(var.dtype, np.number) and var.size > 1]\n",
    "    \n",
    "    if len(numeric_vars) == 0:\n",
    "        print(\"No suitable variables for time series\")\n",
    "        return\n",
    "    \n",
    "    # Plot first few variables\n",
    "    vars_to_plot = numeric_vars[:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(vars_to_plot), 1, figsize=(12, 3*len(vars_to_plot)))\n",
    "    if len(vars_to_plot) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, var_name in enumerate(vars_to_plot):\n",
    "        try:\n",
    "            if time_var in ds[var_name].dims:\n",
    "                ds[var_name].plot(ax=axes[i])\n",
    "            else:\n",
    "                # Plot against time coordinate\n",
    "                axes[i].plot(ds[time_var], ds[var_name])\n",
    "                axes[i].set_xlabel(time_var)\n",
    "                axes[i].set_ylabel(var_name)\n",
    "            \n",
    "            axes[i].set_title(f'{var_name} vs {time_var}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            axes[i].text(0.5, 0.5, f'Error: {str(e)[:50]}...', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
